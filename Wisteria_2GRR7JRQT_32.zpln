{
  "paragraphs": [
    {
      "text": "1 + 1",
      "user": "anonymous",
      "dateUpdated": "2021-12-13T21:15:27+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mres3\u001b[0m: \u001b[1m\u001b[32mInt\u001b[0m = 2\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1639429984462_1106561890",
      "id": "paragraph_1639142597895_646931655",
      "dateCreated": "2021-12-13T21:13:04+0000",
      "status": "FINISHED",
      "focus": true,
      "$$hashKey": "object:244",
      "dateFinished": "2021-12-13T21:15:28+0000",
      "dateStarted": "2021-12-13T21:15:27+0000"
    },
    {
      "text": "%pyspark\n#Carrega dataset já limpo\nrdd = sc.textFile(\"s3://megadados-alunos/dados/all_reviews_clean_tsv\")\nrdd = rdd.cache()",
      "user": "anonymous",
      "dateUpdated": "2021-12-13T21:15:28+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1639429984462_138148096",
      "id": "paragraph_1639143734128_1487039275",
      "dateCreated": "2021-12-13T21:13:04+0000",
      "status": "FINISHED",
      "$$hashKey": "object:245",
      "dateFinished": "2021-12-13T21:15:28+0000",
      "dateStarted": "2021-12-13T21:15:28+0000"
    },
    {
      "text": "%pyspark\n",
      "user": "anonymous",
      "dateUpdated": "2021-12-13T21:15:28+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1639429984462_2100343387",
      "id": "paragraph_1639416063797_1942903095",
      "dateCreated": "2021-12-13T21:13:04+0000",
      "status": "FINISHED",
      "$$hashKey": "object:246",
      "dateFinished": "2021-12-13T21:15:28+0000",
      "dateStarted": "2021-12-13T21:15:28+0000"
    },
    {
      "text": "%pyspark\n#Conta quantas reviews tem\nrdd.count()",
      "user": "anonymous",
      "dateUpdated": "2021-12-13T21:15:28+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "150962278\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://ip-172-31-49-180.ec2.internal:4040/jobs/job?id=32",
              "$$hashKey": "object:3199"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1639429984462_442478044",
      "id": "paragraph_1639144502703_1583629578",
      "dateCreated": "2021-12-13T21:13:04+0000",
      "status": "FINISHED",
      "$$hashKey": "object:247",
      "dateFinished": "2021-12-13T21:16:52+0000",
      "dateStarted": "2021-12-13T21:15:28+0000"
    },
    {
      "text": "%pyspark\n#Exemplo\nresult = rdd \\\n    .map(lambda x: x.split('\\t')) \\\n    .cache()",
      "user": "anonymous",
      "dateUpdated": "2021-12-13T21:16:52+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1639429984462_623133871",
      "id": "paragraph_1639144585923_1335217211",
      "dateCreated": "2021-12-13T21:13:04+0000",
      "status": "FINISHED",
      "$$hashKey": "object:248",
      "dateFinished": "2021-12-13T21:16:52+0000",
      "dateStarted": "2021-12-13T21:16:52+0000"
    },
    {
      "text": "%pyspark\n#Conta quantos clientes tem\nrdd_clients = rdd \\\n    .map(lambda x: x.split(\"\\t\")) \\\n    .map(lambda x: (x[1], 1)) \\\n    .reduceByKey(lambda x, y: x + y) \\\n    .cache()",
      "user": "anonymous",
      "dateUpdated": "2021-12-13T21:16:52+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1639429984463_632142056",
      "id": "paragraph_1639145056493_2083155597",
      "dateCreated": "2021-12-13T21:13:04+0000",
      "status": "FINISHED",
      "$$hashKey": "object:249",
      "dateFinished": "2021-12-13T21:16:52+0000",
      "dateStarted": "2021-12-13T21:16:52+0000"
    },
    {
      "text": "%pyspark\n#Mostra quantos clientes tem\nrdd_clients.count()",
      "user": "anonymous",
      "dateUpdated": "2021-12-13T21:16:52+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "33497620\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://ip-172-31-49-180.ec2.internal:4040/jobs/job?id=33",
              "$$hashKey": "object:3240"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1639429984463_672301527",
      "id": "paragraph_1639146877153_723955111",
      "dateCreated": "2021-12-13T21:13:04+0000",
      "status": "FINISHED",
      "$$hashKey": "object:250",
      "dateFinished": "2021-12-13T21:18:18+0000",
      "dateStarted": "2021-12-13T21:16:52+0000"
    },
    {
      "text": "%pyspark\n#Conta quantos produtos tem\nrdd_products = rdd \\\n    .map(lambda x: x.split(\"\\t\")) \\\n    .map(lambda x: (x[5], 1)) \\\n    .reduceByKey(lambda x, y: x + y) \\\n    .cache()",
      "user": "anonymous",
      "dateUpdated": "2021-12-13T21:18:18+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1639429984463_45907489",
      "id": "paragraph_1639147508196_1368245070",
      "dateCreated": "2021-12-13T21:13:04+0000",
      "status": "FINISHED",
      "$$hashKey": "object:251",
      "dateFinished": "2021-12-13T21:18:19+0000",
      "dateStarted": "2021-12-13T21:18:18+0000"
    },
    {
      "text": "%pyspark\n#Mostra quantos produtos tem\nrdd_products.count()",
      "user": "anonymous",
      "dateUpdated": "2021-12-13T21:18:19+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "15651950\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://ip-172-31-49-180.ec2.internal:4040/jobs/job?id=34",
              "$$hashKey": "object:3281"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1639429984463_836336938",
      "id": "paragraph_1639147775267_190696924",
      "dateCreated": "2021-12-13T21:13:04+0000",
      "status": "FINISHED",
      "$$hashKey": "object:252",
      "dateFinished": "2021-12-13T21:19:35+0000",
      "dateStarted": "2021-12-13T21:18:19+0000"
    },
    {
      "text": "%pyspark\n#Teste\nrdd.take(1)",
      "user": "anonymous",
      "dateUpdated": "2021-12-13T21:19:35+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "['US\\t3653882\\tR3O9SGZBVQBV76\\tB00FALQ1ZC\\t937001370\\tInvicta Women\\'s 15150 \"Angel\" 18k Yellow Gold Ion-Plated Stainless Steel and Brown Leather Watch\\tWatches\\t5\\t0\\t0\\tN\\tY\\tFive Stars\\tAbsolutely love this watch! Get compliments almost every time I wear it. Dainty.\\t2015-08-31']\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://ip-172-31-49-180.ec2.internal:4040/jobs/job?id=35",
              "$$hashKey": "object:3322"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1639429984464_89349329",
      "id": "paragraph_1639147782432_116182404",
      "dateCreated": "2021-12-13T21:13:04+0000",
      "status": "FINISHED",
      "$$hashKey": "object:253",
      "dateFinished": "2021-12-13T21:19:36+0000",
      "dateStarted": "2021-12-13T21:19:35+0000"
    },
    {
      "text": "%pyspark\n#Conta quantas avaliações tem por cada avaliação de estrelas\nrdd_5 = rdd \\\n    .map(lambda x: x.split(\"\\t\")) \\\n    .map(lambda x: (x[7], 1)) \\\n    .reduceByKey(lambda x, y: x + y) \\\n    .cache()",
      "user": "anonymous",
      "dateUpdated": "2021-12-13T21:19:36+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1639429984464_960935682",
      "id": "paragraph_1639148366271_1463684680",
      "dateCreated": "2021-12-13T21:13:04+0000",
      "status": "FINISHED",
      "$$hashKey": "object:254",
      "dateFinished": "2021-12-13T21:19:36+0000",
      "dateStarted": "2021-12-13T21:19:36+0000"
    },
    {
      "text": "%pyspark\n#Mostra contagem de avaliações por star rating. Número maior que 5 para verificar se existe lixo na contagem. Tendo somente cinco, então está limpo, o que significa que os números estão corretos.\nrdd_5.take(10)",
      "user": "anonymous",
      "dateUpdated": "2021-12-13T21:19:36+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "[('2', 7304430), ('4', 26223470), ('5', 93200812), ('3', 12133927), ('1', 12099639)]\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://ip-172-31-49-180.ec2.internal:4040/jobs/job?id=36",
              "$$hashKey": "object:3433"
            },
            {
              "jobUrl": "http://ip-172-31-49-180.ec2.internal:4040/jobs/job?id=37",
              "$$hashKey": "object:3434"
            },
            {
              "jobUrl": "http://ip-172-31-49-180.ec2.internal:4040/jobs/job?id=38",
              "$$hashKey": "object:3435"
            },
            {
              "jobUrl": "http://ip-172-31-49-180.ec2.internal:4040/jobs/job?id=39",
              "$$hashKey": "object:3436"
            },
            {
              "jobUrl": "http://ip-172-31-49-180.ec2.internal:4040/jobs/job?id=40",
              "$$hashKey": "object:3437"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1639429984464_1044789011",
      "id": "paragraph_1639148537424_1194882440",
      "dateCreated": "2021-12-13T21:13:04+0000",
      "status": "FINISHED",
      "$$hashKey": "object:255",
      "dateFinished": "2021-12-13T21:20:31+0000",
      "dateStarted": "2021-12-13T21:19:36+0000"
    },
    {
      "text": "%pyspark\n#Verifica top 10 produtos avaliados por clientes que apenas avaliaram uma vez\nrdd_top10 = rdd \\\n    .map(lambda x: x.split(\"\\t\")) ",
      "user": "anonymous",
      "dateUpdated": "2021-12-13T21:20:31+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1639429984465_1804754078",
      "id": "paragraph_1639148546150_573371074",
      "dateCreated": "2021-12-13T21:13:04+0000",
      "status": "FINISHED",
      "$$hashKey": "object:256",
      "dateFinished": "2021-12-13T21:20:31+0000",
      "dateStarted": "2021-12-13T21:20:31+0000"
    },
    {
      "text": "%pyspark\n#verifica por bots\nrdd_clients.takeOrdered(2, lambda x: -x[1])",
      "user": "anonymous",
      "dateUpdated": "2021-12-13T21:20:31+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "[('50122160', 59623), ('50732546', 30761)]\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://ip-172-31-49-180.ec2.internal:4040/jobs/job?id=41",
              "$$hashKey": "object:3494"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1639429984465_977270457",
      "id": "paragraph_1639168702731_1200243388",
      "dateCreated": "2021-12-13T21:13:04+0000",
      "status": "FINISHED",
      "$$hashKey": "object:257",
      "dateFinished": "2021-12-13T21:20:38+0000",
      "dateStarted": "2021-12-13T21:20:31+0000"
    },
    {
      "text": "%pyspark\n#Cria dataframe\n\ndf = spark.read.option(\"header\", \"false\") \\\n    .option(\"delimiter\", \"\\t\") \\\n    .option(\"inferSchema\", \"true\") \\\n    .csv(\"s3://megadados-alunos/dados/all_reviews_clean_tsv\") \\\n    .withColumnRenamed(\"_c0\", \"marketplace\") \\\n    .withColumnRenamed(\"_c1\", \"customer_id\") \\\n    .withColumnRenamed(\"_c2\", \"review_id\") \\\n    .withColumnRenamed(\"_c3\", \"product_id\") \\\n    .withColumnRenamed(\"_c4\", \"product_parent\") \\\n    .withColumnRenamed(\"_c5\", \"product_title\") \\\n    .withColumnRenamed(\"_c6\", \"product_category\") \\\n    .withColumnRenamed(\"_c7\", \"star_rating\") \\\n    .withColumnRenamed(\"_c8\", \"helpful_votes\") \\\n    .withColumnRenamed(\"_c9\", \"total_votes\") \\\n    .withColumnRenamed(\"_c10\", \"vine\") \\\n    .withColumnRenamed(\"_c11\", \"verified_purchase\") \\\n    .withColumnRenamed(\"_c12\", \"review_headline\") \\\n    .withColumnRenamed(\"_c13\", \"review_body\") \\\n    .withColumnRenamed(\"_c14\", \"review_date\") \\\n    ",
      "user": "anonymous",
      "dateUpdated": "2021-12-13T21:20:38+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://ip-172-31-49-180.ec2.internal:4040/jobs/job?id=42",
              "$$hashKey": "object:3539"
            },
            {
              "jobUrl": "http://ip-172-31-49-180.ec2.internal:4040/jobs/job?id=43",
              "$$hashKey": "object:3540"
            },
            {
              "jobUrl": "http://ip-172-31-49-180.ec2.internal:4040/jobs/job?id=44",
              "$$hashKey": "object:3541"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1639429984465_625444681",
      "id": "paragraph_1639168738727_914899143",
      "dateCreated": "2021-12-13T21:13:04+0000",
      "status": "FINISHED",
      "$$hashKey": "object:258",
      "dateFinished": "2021-12-13T21:21:57+0000",
      "dateStarted": "2021-12-13T21:20:38+0000"
    },
    {
      "text": "%pyspark\nunique_customers= df.select(\"customer_id\").distinct()",
      "user": "anonymous",
      "dateUpdated": "2021-12-13T21:21:57+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1639429984465_1558055269",
      "id": "paragraph_1639173119848_1397070466",
      "dateCreated": "2021-12-13T21:13:04+0000",
      "status": "FINISHED",
      "$$hashKey": "object:259",
      "dateFinished": "2021-12-13T21:21:57+0000",
      "dateStarted": "2021-12-13T21:21:57+0000"
    },
    {
      "text": "%pyspark\nunique_customers_count = unique_customers.count()",
      "user": "anonymous",
      "dateUpdated": "2021-12-13T21:21:58+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://ip-172-31-49-180.ec2.internal:4040/jobs/job?id=45",
              "$$hashKey": "object:3599"
            },
            {
              "jobUrl": "http://ip-172-31-49-180.ec2.internal:4040/jobs/job?id=46",
              "$$hashKey": "object:3600"
            },
            {
              "jobUrl": "http://ip-172-31-49-180.ec2.internal:4040/jobs/job?id=47",
              "$$hashKey": "object:3601"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1639429984465_1266624206",
      "id": "paragraph_1639173332554_578094149",
      "dateCreated": "2021-12-13T21:13:04+0000",
      "status": "FINISHED",
      "$$hashKey": "object:260",
      "dateFinished": "2021-12-13T21:23:21+0000",
      "dateStarted": "2021-12-13T21:21:58+0000"
    },
    {
      "text": "%pyspark\none_unique_customer = unique_customers.take(1)",
      "user": "anonymous",
      "dateUpdated": "2021-12-13T21:23:21+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://ip-172-31-49-180.ec2.internal:4040/jobs/job?id=48",
              "$$hashKey": "object:3640"
            },
            {
              "jobUrl": "http://ip-172-31-49-180.ec2.internal:4040/jobs/job?id=49",
              "$$hashKey": "object:3641"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1639429984465_1559949006",
      "id": "paragraph_1639173432367_445816950",
      "dateCreated": "2021-12-13T21:13:04+0000",
      "status": "FINISHED",
      "$$hashKey": "object:261",
      "dateFinished": "2021-12-13T21:24:42+0000",
      "dateStarted": "2021-12-13T21:23:21+0000"
    },
    {
      "text": "%pyspark\n#check_customer = df.filter(df.select(\"customer_id\") == \"52705992\").count()",
      "user": "anonymous",
      "dateUpdated": "2021-12-13T21:24:42+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1639429984465_1427259392",
      "id": "paragraph_1639173459186_2005007966",
      "dateCreated": "2021-12-13T21:13:04+0000",
      "status": "FINISHED",
      "$$hashKey": "object:262",
      "dateFinished": "2021-12-13T21:24:42+0000",
      "dateStarted": "2021-12-13T21:24:42+0000"
    },
    {
      "text": "%pyspark\n#criterio 3\nfrom pyspark.ml.feature import CountVectorizer\nfrom pyspark.ml.feature import Tokenizer, RegexTokenizer\nfrom pyspark.ml.feature import StringIndexer\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.classification import NaiveBayes",
      "user": "anonymous",
      "dateUpdated": "2021-12-13T21:24:42+0000",
      "progress": 0,
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "results": {},
        "enabled": true,
        "editorSetting": {}
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1639429984465_2130928547",
      "id": "paragraph_1639173606408_1838666492",
      "dateCreated": "2021-12-13T21:13:04+0000",
      "status": "FINISHED",
      "$$hashKey": "object:263",
      "dateFinished": "2021-12-13T21:24:42+0000",
      "dateStarted": "2021-12-13T21:24:42+0000"
    },
    {
      "text": "%pyspark\ndf.show()",
      "user": "anonymous",
      "dateUpdated": "2021-12-13T21:24:42+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "+-----------+-----------+--------------+----------+--------------+--------------------+----------------+-----------+-------------+-----------+----+-----------------+--------------------+--------------------+-----------+\n|marketplace|customer_id|     review_id|product_id|product_parent|       product_title|product_category|star_rating|helpful_votes|total_votes|vine|verified_purchase|     review_headline|         review_body|review_date|\n+-----------+-----------+--------------+----------+--------------+--------------------+----------------+-----------+-------------+-----------+----+-----------------+--------------------+--------------------+-----------+\n|         US|   49303758| RN7SOSN3M1R4L|006196140X|     987170434|Dear Cary: My Lif...|           Books|          4|            1|          2|   N|                N|An Honest Reflection|Dyan Cannon, the ...| 2011-11-10|\n|         US|   45359011|R2U6CZ6U4RZZDM|1609761685|     735763337|Rachel and Sammy ...|           Books|          5|            0|          0|   N|                Y|Volunteer at Robi...|The book was in e...| 2011-11-10|\n|         US|   16024217|R3O4NAGN1I56FZ|0953674509|     507368206|Wicca Unveiled: T...|           Books|          3|            0|          0|   N|                N|Something a littl...|This book deals w...| 2011-11-10|\n|         US|   46830129|R2HS1KLEWH307A|1466263687|     550077841|The Ultimate Hang...|           Books|          5|            0|          0|   N|                N|Great resource fo...|I'm still new to ...| 2011-11-10|\n|         US|   35489180|R3MV7VQSN2L2FA|0755381173|     865023266|A Discovery of Wi...|           Books|          5|            1|          4|   N|                N|          Phenominal|Absolutely the mo...| 2011-11-10|\n|         US|   26688061| R8IVK5SDPX7U4|1582613575|     551925365|Touched: The Jerr...|           Books|          1|           22|         35|   N|                N|  Do the right thing|Take this GD book...| 2011-11-10|\n|         US|   35656627| R9TBF84BD10VU|143895512X|     843491995|Your Better Self:...|           Books|          5|            0|          0|   N|                N|The Best Self Hel...|This book is FANT...| 2011-11-10|\n|         US|   16073212|R3JYKCSRXEKSFA|1582613575|     551925365|Touched: The Jerr...|           Books|          1|            7|          7|   N|                N|Wow - How Much Do...|In his own words:...| 2011-11-10|\n|         US|   22431040|R1SH8SY6GO2SF6|1935708449|     994114816|My Life as Laura:...|           Books|          5|            4|          4|   N|                Y|    My Life As Laura|Light, very plesa...| 2011-11-10|\n|         US|   41733776|R1FCYH6VBCHUAY|1250000238|     572290174|Dragon's Oath (Ho...|           Books|          3|            0|          0|   N|                Y|  Wanting more......|It was a nice lit...| 2011-11-10|\n|         US|   35616088| RF8B6DCPGH1J2|1892112000|     246105409| To Train Up a Child|           Books|          5|           25|         50|   N|                N|      great teaching|Love the scriptur...| 2011-11-10|\n|         US|   23240151| RKBC83MABQB9L|1596982926|     322107662|Sweet Land of Lib...|           Books|          5|            2|          6|   N|                N|Love the USA and ...|Sweet Land of Lib...| 2011-11-10|\n|         US|   35433720|R11OR4KLCPD1HH|0060596996|     298424895|Lit: A Memoir (P.S.)|           Books|          4|            1|          2|   N|                N|My view on the wr...|In the book Lit b...| 2011-11-10|\n|         US|   49957928| RDY9WOU6P7IEU|B005SMWSHS|     308348789|       Into the Wild|           Books|          4|            0|          0|   N|                N|Spooky, Scary Fai...|Today's pick is S...| 2011-11-10|\n|         US|   52733753|R2SIB613WR5FBW|1892112000|     246105409| To Train Up a Child|           Books|          5|           30|         59|   N|                N|     Excellent book!|This is an excell...| 2011-11-10|\n|         US|   51803143|R1CCBN0A7BT18I|0878055339|     406222258|Conversations wit...|           Books|          5|            4|          4|   N|                N|May Sarton Discus...|This collection c...| 2011-11-10|\n|         US|   51307077|R3F64XYNELFJAG|0470258233|     122164346|SolidWorks Surfac...|           Books|          2|            4|          6|   N|                Y|Looks Good - but ...|I have used Swork...| 2011-11-10|\n|         US|   24284623|R3QUKBSUT22OM7|1599906813|     332116521|     After Obsession|           Books|          2|            0|          1|   N|                N|After Obsession R...|This book was a l...| 2011-11-10|\n|         US|   45616010| RVIPVQSVUMNT8|0316056197|     517014463|        Ship Breaker|           Books|          5|            0|          0|   N|                Y| Another great novel|Paolo Bacigalupi ...| 2011-11-10|\n|         US|   13200166| RSXXT9K1JOXZE|0312622082|     982992183|Gypsy Boy: My Lif...|           Books|          5|            4|          5|   Y|                N| Not what I expected|I was expecting t...| 2011-11-10|\n+-----------+-----------+--------------+----------+--------------+--------------------+----------------+-----------+-------------+-----------+----+-----------------+--------------------+--------------------+-----------+\nonly showing top 20 rows\n\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://ip-172-31-49-180.ec2.internal:4040/jobs/job?id=50",
              "$$hashKey": "object:3665"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1639429984465_1252069445",
      "id": "paragraph_1639414849123_2053754483",
      "dateCreated": "2021-12-13T21:13:04+0000",
      "status": "FINISHED",
      "$$hashKey": "object:264",
      "dateFinished": "2021-12-13T21:24:42+0000",
      "dateStarted": "2021-12-13T21:24:42+0000"
    },
    {
      "text": "%pyspark\nfrom pyspark.sql.functions import when\nfrom pyspark.sql.functions import regexp_replace\n\ndf = df.withColumn(\"reception\", df[\"star_rating\"])\ndf = df.withColumn('reception', \n    when(df.reception.endswith('5'),regexp_replace(df.reception,'5','positive')) \\\n   .when(df.reception.endswith('4'),regexp_replace(df.reception,'4','neutral')) \\\n   .when(df.reception.endswith('3'),regexp_replace(df.reception,'3','negative')) \\\n   .when(df.reception.endswith('2'),regexp_replace(df.reception,'2','negative')) \\\n   .when(df.reception.endswith('1'),regexp_replace(df.reception,'1','negative')) \\\n   .when(df.reception.endswith('0'),regexp_replace(df.reception,'0','negative')) \\\n   .otherwise(df.reception))\n   \ndf = df.na.drop(subset=[\"review_headline\",\"star_rating\",\"reception\"])\n",
      "user": "anonymous",
      "dateUpdated": "2021-12-13T21:24:42+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1639429984465_939332831",
      "id": "paragraph_1639414863931_1778567502",
      "dateCreated": "2021-12-13T21:13:04+0000",
      "status": "FINISHED",
      "$$hashKey": "object:265",
      "dateFinished": "2021-12-13T21:24:42+0000",
      "dateStarted": "2021-12-13T21:24:42+0000"
    },
    {
      "text": "%pyspark\n\ndf.show()",
      "user": "anonymous",
      "dateUpdated": "2021-12-13T21:24:42+0000",
      "progress": 0,
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "results": {},
        "enabled": true,
        "editorSetting": {}
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "+-----------+-----------+--------------+----------+--------------+--------------------+----------------+-----------+-------------+-----------+----+-----------------+--------------------+--------------------+-----------+---------+\n|marketplace|customer_id|     review_id|product_id|product_parent|       product_title|product_category|star_rating|helpful_votes|total_votes|vine|verified_purchase|     review_headline|         review_body|review_date|reception|\n+-----------+-----------+--------------+----------+--------------+--------------------+----------------+-----------+-------------+-----------+----+-----------------+--------------------+--------------------+-----------+---------+\n|         US|   49303758| RN7SOSN3M1R4L|006196140X|     987170434|Dear Cary: My Lif...|           Books|          4|            1|          2|   N|                N|An Honest Reflection|Dyan Cannon, the ...| 2011-11-10|  neutral|\n|         US|   45359011|R2U6CZ6U4RZZDM|1609761685|     735763337|Rachel and Sammy ...|           Books|          5|            0|          0|   N|                Y|Volunteer at Robi...|The book was in e...| 2011-11-10| positive|\n|         US|   16024217|R3O4NAGN1I56FZ|0953674509|     507368206|Wicca Unveiled: T...|           Books|          3|            0|          0|   N|                N|Something a littl...|This book deals w...| 2011-11-10| negative|\n|         US|   46830129|R2HS1KLEWH307A|1466263687|     550077841|The Ultimate Hang...|           Books|          5|            0|          0|   N|                N|Great resource fo...|I'm still new to ...| 2011-11-10| positive|\n|         US|   35489180|R3MV7VQSN2L2FA|0755381173|     865023266|A Discovery of Wi...|           Books|          5|            1|          4|   N|                N|          Phenominal|Absolutely the mo...| 2011-11-10| positive|\n|         US|   26688061| R8IVK5SDPX7U4|1582613575|     551925365|Touched: The Jerr...|           Books|          1|           22|         35|   N|                N|  Do the right thing|Take this GD book...| 2011-11-10| negative|\n|         US|   35656627| R9TBF84BD10VU|143895512X|     843491995|Your Better Self:...|           Books|          5|            0|          0|   N|                N|The Best Self Hel...|This book is FANT...| 2011-11-10| positive|\n|         US|   16073212|R3JYKCSRXEKSFA|1582613575|     551925365|Touched: The Jerr...|           Books|          1|            7|          7|   N|                N|Wow - How Much Do...|In his own words:...| 2011-11-10| negative|\n|         US|   22431040|R1SH8SY6GO2SF6|1935708449|     994114816|My Life as Laura:...|           Books|          5|            4|          4|   N|                Y|    My Life As Laura|Light, very plesa...| 2011-11-10| positive|\n|         US|   41733776|R1FCYH6VBCHUAY|1250000238|     572290174|Dragon's Oath (Ho...|           Books|          3|            0|          0|   N|                Y|  Wanting more......|It was a nice lit...| 2011-11-10| negative|\n|         US|   35616088| RF8B6DCPGH1J2|1892112000|     246105409| To Train Up a Child|           Books|          5|           25|         50|   N|                N|      great teaching|Love the scriptur...| 2011-11-10| positive|\n|         US|   23240151| RKBC83MABQB9L|1596982926|     322107662|Sweet Land of Lib...|           Books|          5|            2|          6|   N|                N|Love the USA and ...|Sweet Land of Lib...| 2011-11-10| positive|\n|         US|   35433720|R11OR4KLCPD1HH|0060596996|     298424895|Lit: A Memoir (P.S.)|           Books|          4|            1|          2|   N|                N|My view on the wr...|In the book Lit b...| 2011-11-10|  neutral|\n|         US|   49957928| RDY9WOU6P7IEU|B005SMWSHS|     308348789|       Into the Wild|           Books|          4|            0|          0|   N|                N|Spooky, Scary Fai...|Today's pick is S...| 2011-11-10|  neutral|\n|         US|   52733753|R2SIB613WR5FBW|1892112000|     246105409| To Train Up a Child|           Books|          5|           30|         59|   N|                N|     Excellent book!|This is an excell...| 2011-11-10| positive|\n|         US|   51803143|R1CCBN0A7BT18I|0878055339|     406222258|Conversations wit...|           Books|          5|            4|          4|   N|                N|May Sarton Discus...|This collection c...| 2011-11-10| positive|\n|         US|   51307077|R3F64XYNELFJAG|0470258233|     122164346|SolidWorks Surfac...|           Books|          2|            4|          6|   N|                Y|Looks Good - but ...|I have used Swork...| 2011-11-10| negative|\n|         US|   24284623|R3QUKBSUT22OM7|1599906813|     332116521|     After Obsession|           Books|          2|            0|          1|   N|                N|After Obsession R...|This book was a l...| 2011-11-10| negative|\n|         US|   45616010| RVIPVQSVUMNT8|0316056197|     517014463|        Ship Breaker|           Books|          5|            0|          0|   N|                Y| Another great novel|Paolo Bacigalupi ...| 2011-11-10| positive|\n|         US|   13200166| RSXXT9K1JOXZE|0312622082|     982992183|Gypsy Boy: My Lif...|           Books|          5|            4|          5|   Y|                N| Not what I expected|I was expecting t...| 2011-11-10| positive|\n+-----------+-----------+--------------+----------+--------------+--------------------+----------------+-----------+-------------+-----------+----+-----------------+--------------------+--------------------+-----------+---------+\nonly showing top 20 rows\n\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://ip-172-31-49-180.ec2.internal:4040/jobs/job?id=51",
              "$$hashKey": "object:3688"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1639429984466_726640863",
      "id": "paragraph_1639174187272_1995037936",
      "dateCreated": "2021-12-13T21:13:04+0000",
      "status": "FINISHED",
      "$$hashKey": "object:266",
      "dateFinished": "2021-12-13T21:24:43+0000",
      "dateStarted": "2021-12-13T21:24:42+0000"
    },
    {
      "text": "%pyspark\nstages = []\n# 1. clean data and tokenize sentences using RegexTokenizer\nregexTokenizer = RegexTokenizer(inputCol=\"review_headline\", outputCol=\"tokens\", pattern=\"\\\\W+\")\nstages.append(regexTokenizer)\n\nprint(regexTokenizer)\n\n# 2. CountVectorize the data\ncv = CountVectorizer(inputCol=\"tokens\", outputCol=\"token_features\", minDF=2.0)#, vocabSize=3, minDF=2.0\nstages.append(cv)\n\n# 3. Convert the labels to numerical values using binariser\nindexer = StringIndexer(inputCol=\"reception\", outputCol=\"label\")\nstages.append(indexer)\n\n# 4. Vectorise features using vectorassembler\nvecAssembler = VectorAssembler(inputCols=['token_features'], outputCol=\"features\")\nstages.append(vecAssembler)\n\nprint(stages)\n#[print('\\n', stage) for stage in stages]",
      "user": "anonymous",
      "dateUpdated": "2021-12-13T21:24:43+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "RegexTokenizer_567aa38d4e37\n[RegexTokenizer_567aa38d4e37, CountVectorizer_5042d47d9e7b, StringIndexer_f074ff18f259, VectorAssembler_73634e55c46e]\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1639429984466_51707434",
      "id": "paragraph_1639418328437_1836718488",
      "dateCreated": "2021-12-13T21:13:04+0000",
      "status": "FINISHED",
      "$$hashKey": "object:267",
      "dateFinished": "2021-12-13T21:24:43+0000",
      "dateStarted": "2021-12-13T21:24:43+0000"
    },
    {
      "text": "%pyspark\n\nfrom pyspark.ml import Pipeline\npipeline = Pipeline(stages=stages)\ndata = pipeline.fit(df).transform(df)\n",
      "user": "anonymous",
      "dateUpdated": "2021-12-13T21:24:43+0000",
      "progress": 0,
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "results": {},
        "enabled": true,
        "editorSetting": {}
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://ip-172-31-49-180.ec2.internal:4040/jobs/job?id=52",
              "$$hashKey": "object:3751"
            },
            {
              "jobUrl": "http://ip-172-31-49-180.ec2.internal:4040/jobs/job?id=53",
              "$$hashKey": "object:3752"
            },
            {
              "jobUrl": "http://ip-172-31-49-180.ec2.internal:4040/jobs/job?id=54",
              "$$hashKey": "object:3753"
            },
            {
              "jobUrl": "http://ip-172-31-49-180.ec2.internal:4040/jobs/job?id=55",
              "$$hashKey": "object:3754"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1639429984466_831230571",
      "id": "paragraph_1639174189583_953135858",
      "dateCreated": "2021-12-13T21:13:04+0000",
      "status": "FINISHED",
      "$$hashKey": "object:268",
      "dateFinished": "2021-12-13T21:28:08+0000",
      "dateStarted": "2021-12-13T21:24:43+0000"
    },
    {
      "text": "%pyspark\n\ntrain, test = data.randomSplit([0.7, 0.3], seed = 2018)\n",
      "user": "anonymous",
      "dateUpdated": "2021-12-13T21:28:08+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1639429984466_1363273270",
      "id": "paragraph_1639418335176_456987470",
      "dateCreated": "2021-12-13T21:13:04+0000",
      "status": "FINISHED",
      "$$hashKey": "object:269",
      "dateFinished": "2021-12-13T21:28:08+0000",
      "dateStarted": "2021-12-13T21:28:08+0000"
    },
    {
      "text": "%pyspark\nfrom pyspark.ml.classification import NaiveBayes\n# Initialise the model\nnb = NaiveBayes(smoothing=1.0, modelType=\"multinomial\")\n# Fit the model\nmodel = nb.fit(train)\n# Make predictions on test data\npredictions = model.transform(test)\npredictions.select(\"label\", \"prediction\", \"probability\").show()",
      "user": "anonymous",
      "dateUpdated": "2021-12-13T21:28:08+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "Py4JJavaError: An error occurred while calling o1302.fit.\n: java.lang.NullPointerException\n\tat org.apache.spark.sql.execution.DataSourceScanExec.$init$(DataSourceScanExec.scala:61)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.<init>(DataSourceScanExec.scala:176)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.doCanonicalize(DataSourceScanExec.scala:762)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.doCanonicalize(DataSourceScanExec.scala:166)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.canonicalized$lzycompute(QueryPlan.scala:373)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.canonicalized(QueryPlan.scala:372)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$doCanonicalize$1(QueryPlan.scala:387)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.immutable.List.map(List.scala:298)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.doCanonicalize(QueryPlan.scala:387)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.canonicalized$lzycompute(QueryPlan.scala:373)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.canonicalized(QueryPlan.scala:372)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$doCanonicalize$1(QueryPlan.scala:387)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.immutable.List.map(List.scala:298)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.doCanonicalize(QueryPlan.scala:387)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.canonicalized$lzycompute(QueryPlan.scala:373)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.canonicalized(QueryPlan.scala:372)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$doCanonicalize$1(QueryPlan.scala:387)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.immutable.List.map(List.scala:298)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.doCanonicalize(QueryPlan.scala:387)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.canonicalized$lzycompute(QueryPlan.scala:373)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.canonicalized(QueryPlan.scala:372)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$doCanonicalize$1(QueryPlan.scala:387)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.immutable.List.map(List.scala:298)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.doCanonicalize(QueryPlan.scala:387)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.canonicalized$lzycompute(QueryPlan.scala:373)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.canonicalized(QueryPlan.scala:372)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$doCanonicalize$1(QueryPlan.scala:387)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.immutable.List.map(List.scala:298)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.doCanonicalize(QueryPlan.scala:387)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.canonicalized$lzycompute(QueryPlan.scala:373)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.canonicalized(QueryPlan.scala:372)\n\tat org.apache.spark.sql.execution.SortExec.genCodeForIterator(SortExec.scala:178)\n\tat org.apache.spark.sql.execution.SortExec.doProduce(SortExec.scala:287)\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:97)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:232)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:229)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:92)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:92)\n\tat org.apache.spark.sql.execution.SortExec.produce(SortExec.scala:41)\n\tat org.apache.spark.sql.execution.SampleExec.doProduce(basicPhysicalOperators.scala:360)\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:97)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:232)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:229)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:92)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:92)\n\tat org.apache.spark.sql.execution.SampleExec.produce(basicPhysicalOperators.scala:324)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen(WholeStageCodegenExec.scala:801)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:870)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:194)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:232)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:229)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:190)\n\tat org.apache.spark.sql.execution.DeserializeToObjectExec.doExecute(objects.scala:96)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:194)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:232)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:229)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:190)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:134)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:133)\n\tat org.apache.spark.sql.Dataset$RDDQueryExecution.rdd$lzycompute(Dataset.scala:3839)\n\tat org.apache.spark.sql.Dataset$RDDQueryExecution.rdd(Dataset.scala:3837)\n\tat org.apache.spark.sql.Dataset.rdd$lzycompute(Dataset.scala:3285)\n\tat org.apache.spark.sql.Dataset.rdd(Dataset.scala:3278)\n\tat org.apache.spark.ml.util.Instrumentation.logDataset(Instrumentation.scala:62)\n\tat org.apache.spark.ml.classification.NaiveBayes.$anonfun$trainWithLabelCheck$1(NaiveBayes.scala:146)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n\tat org.apache.spark.ml.classification.NaiveBayes.trainWithLabelCheck(NaiveBayes.scala:144)\n\tat org.apache.spark.ml.classification.NaiveBayes.train(NaiveBayes.scala:133)\n\tat org.apache.spark.ml.classification.NaiveBayes.train(NaiveBayes.scala:95)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:151)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:115)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n\n(<class 'py4j.protocol.Py4JJavaError'>, Py4JJavaError('An error occurred while calling o1302.fit.\\n', JavaObject id=o1326), <traceback object at 0x7fa7bbf0b5f0>)"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1639429984466_800868355",
      "id": "paragraph_1639174258127_1015368380",
      "dateCreated": "2021-12-13T21:13:04+0000",
      "status": "ERROR",
      "$$hashKey": "object:270",
      "dateFinished": "2021-12-13T21:28:08+0000",
      "dateStarted": "2021-12-13T21:28:08+0000"
    },
    {
      "text": "%pyspark\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\nevaluator = MultiClassificationEvaluator(rawPredictionCol=\"prediction\")\naccuracy = evaluator.evaluate(predictions)\nprint (\"Model Accuracy: \", accuracy)",
      "user": "anonymous",
      "dateUpdated": "2021-12-13T21:40:47+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1639429984466_142738941",
      "id": "paragraph_1639418425961_330308214",
      "dateCreated": "2021-12-13T21:13:04+0000",
      "status": "READY",
      "$$hashKey": "object:271"
    },
    {
      "text": "%pyspark\nfrom pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n\n# Create ParamGrid and Evaluator for Cross Validation\nparamGrid = ParamGridBuilder().addGrid(nb.smoothing, [0.0, 0.2, 0.4, 0.6, 0.8, 1.0, 1.5, 2.0]).build()\ncvEvaluator = MultiClassificationEvaluator(rawPredictionCol=\"prediction\")\n\n# Run Cross-validation\ncv = CrossValidator(estimator=nb, estimatorParamMaps=paramGrid, evaluator=cvEvaluator)\ncvModel = cv.fit(train)\n\n# Make predictions on testData. cvModel uses the bestModel.\ncvPredictions = cvModel.transform(test)\n\n# Evaluate bestModel found from Cross Validation\nevaluator.evaluate(cvPredictions)",
      "user": "anonymous",
      "dateUpdated": "2021-12-13T21:40:46+0000",
      "progress": 0,
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "results": {},
        "enabled": true,
        "editorSetting": {}
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1639429984466_1710362303",
      "id": "paragraph_1639412156845_5925627",
      "dateCreated": "2021-12-13T21:13:04+0000",
      "status": "READY",
      "$$hashKey": "object:272"
    },
    {
      "text": "%pyspark",
      "user": "anonymous",
      "dateUpdated": "2021-12-13T21:15:01+0000",
      "progress": 0,
      "config": {
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "results": {},
        "enabled": true,
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1639429984466_339599917",
      "id": "paragraph_1639412160257_1475284230",
      "dateCreated": "2021-12-13T21:13:04+0000",
      "status": "READY",
      "$$hashKey": "object:273"
    },
    {
      "text": "%pyspark\n",
      "user": "anonymous",
      "dateUpdated": "2021-12-13T21:15:11+0000",
      "progress": 0,
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "results": {},
        "enabled": true,
        "editorSetting": {}
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "NaiveBayes_62b5ed6a365c\nDataFrame[marketplace: string, customer_id: int, review_id: string, product_id: string, product_parent: int, product_title: string, product_category: string, star_rating: string, helpful_votes: int, total_votes: int, vine: string, verified_purchase: string, review_headline: string, review_body: string, review_date: string, tokens: array<string>, token_features: vector, label: double, features: vector]\n"
          },
          {
            "type": "TEXT",
            "data": "Py4JJavaError: An error occurred while calling o5345.fit.\n: java.lang.NullPointerException\n\tat org.apache.spark.sql.execution.DataSourceScanExec.$init$(DataSourceScanExec.scala:61)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.<init>(DataSourceScanExec.scala:176)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.doCanonicalize(DataSourceScanExec.scala:762)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.doCanonicalize(DataSourceScanExec.scala:166)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.canonicalized$lzycompute(QueryPlan.scala:373)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.canonicalized(QueryPlan.scala:372)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$doCanonicalize$1(QueryPlan.scala:387)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.immutable.List.map(List.scala:298)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.doCanonicalize(QueryPlan.scala:387)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.canonicalized$lzycompute(QueryPlan.scala:373)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.canonicalized(QueryPlan.scala:372)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$doCanonicalize$1(QueryPlan.scala:387)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.immutable.List.map(List.scala:298)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.doCanonicalize(QueryPlan.scala:387)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.canonicalized$lzycompute(QueryPlan.scala:373)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.canonicalized(QueryPlan.scala:372)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$doCanonicalize$1(QueryPlan.scala:387)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.immutable.List.map(List.scala:298)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.doCanonicalize(QueryPlan.scala:387)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.canonicalized$lzycompute(QueryPlan.scala:373)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.canonicalized(QueryPlan.scala:372)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$doCanonicalize$1(QueryPlan.scala:387)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.immutable.List.map(List.scala:298)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.doCanonicalize(QueryPlan.scala:387)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.canonicalized$lzycompute(QueryPlan.scala:373)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.canonicalized(QueryPlan.scala:372)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$doCanonicalize$1(QueryPlan.scala:387)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.immutable.List.map(List.scala:298)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.doCanonicalize(QueryPlan.scala:387)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.canonicalized$lzycompute(QueryPlan.scala:373)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.canonicalized(QueryPlan.scala:372)\n\tat org.apache.spark.sql.execution.SortExec.genCodeForIterator(SortExec.scala:178)\n\tat org.apache.spark.sql.execution.SortExec.doProduce(SortExec.scala:287)\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:97)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:232)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:229)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:92)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:92)\n\tat org.apache.spark.sql.execution.SortExec.produce(SortExec.scala:41)\n\tat org.apache.spark.sql.execution.SampleExec.doProduce(basicPhysicalOperators.scala:360)\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:97)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:232)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:229)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:92)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:92)\n\tat org.apache.spark.sql.execution.SampleExec.produce(basicPhysicalOperators.scala:324)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen(WholeStageCodegenExec.scala:801)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:870)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:194)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:232)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:229)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:190)\n\tat org.apache.spark.sql.execution.DeserializeToObjectExec.doExecute(objects.scala:96)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:194)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:232)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:229)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:190)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:134)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:133)\n\tat org.apache.spark.sql.Dataset$RDDQueryExecution.rdd$lzycompute(Dataset.scala:3839)\n\tat org.apache.spark.sql.Dataset$RDDQueryExecution.rdd(Dataset.scala:3837)\n\tat org.apache.spark.sql.Dataset.rdd$lzycompute(Dataset.scala:3285)\n\tat org.apache.spark.sql.Dataset.rdd(Dataset.scala:3278)\n\tat org.apache.spark.ml.util.Instrumentation.logDataset(Instrumentation.scala:62)\n\tat org.apache.spark.ml.classification.NaiveBayes.$anonfun$trainWithLabelCheck$1(NaiveBayes.scala:146)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n\tat org.apache.spark.ml.classification.NaiveBayes.trainWithLabelCheck(NaiveBayes.scala:144)\n\tat org.apache.spark.ml.classification.NaiveBayes.train(NaiveBayes.scala:133)\n\tat org.apache.spark.ml.classification.NaiveBayes.train(NaiveBayes.scala:95)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:151)\n\tat sun.reflect.GeneratedMethodAccessor453.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n\n(<class 'py4j.protocol.Py4JJavaError'>, Py4JJavaError('An error occurred while calling o5345.fit.\\n', JavaObject id=o5371), <traceback object at 0x7f101ff19f50>)"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1639429984467_1689301466",
      "id": "paragraph_1639412145573_253036165",
      "dateCreated": "2021-12-13T21:13:04+0000",
      "status": "READY",
      "$$hashKey": "object:274"
    },
    {
      "text": "%pyspark\n",
      "user": "anonymous",
      "dateUpdated": "2021-12-13T21:15:20+0000",
      "progress": 0,
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "results": {},
        "enabled": true,
        "editorSetting": {}
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "Py4JJavaError: An error occurred while calling o5325.evaluate.\n: java.lang.NullPointerException\n\tat org.apache.spark.sql.execution.DataSourceScanExec.$init$(DataSourceScanExec.scala:61)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.<init>(DataSourceScanExec.scala:176)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.doCanonicalize(DataSourceScanExec.scala:762)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.doCanonicalize(DataSourceScanExec.scala:166)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.canonicalized$lzycompute(QueryPlan.scala:373)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.canonicalized(QueryPlan.scala:372)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$doCanonicalize$1(QueryPlan.scala:387)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.immutable.List.map(List.scala:298)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.doCanonicalize(QueryPlan.scala:387)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.canonicalized$lzycompute(QueryPlan.scala:373)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.canonicalized(QueryPlan.scala:372)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$doCanonicalize$1(QueryPlan.scala:387)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.immutable.List.map(List.scala:298)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.doCanonicalize(QueryPlan.scala:387)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.canonicalized$lzycompute(QueryPlan.scala:373)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.canonicalized(QueryPlan.scala:372)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$doCanonicalize$1(QueryPlan.scala:387)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.immutable.List.map(List.scala:298)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.doCanonicalize(QueryPlan.scala:387)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.canonicalized$lzycompute(QueryPlan.scala:373)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.canonicalized(QueryPlan.scala:372)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$doCanonicalize$1(QueryPlan.scala:387)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.immutable.List.map(List.scala:298)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.doCanonicalize(QueryPlan.scala:387)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.canonicalized$lzycompute(QueryPlan.scala:373)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.canonicalized(QueryPlan.scala:372)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$doCanonicalize$1(QueryPlan.scala:387)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.immutable.List.map(List.scala:298)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.doCanonicalize(QueryPlan.scala:387)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.canonicalized$lzycompute(QueryPlan.scala:373)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.canonicalized(QueryPlan.scala:372)\n\tat org.apache.spark.sql.execution.SortExec.genCodeForIterator(SortExec.scala:178)\n\tat org.apache.spark.sql.execution.SortExec.doProduce(SortExec.scala:287)\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:97)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:232)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:229)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:92)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:92)\n\tat org.apache.spark.sql.execution.SortExec.produce(SortExec.scala:41)\n\tat org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:54)\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:97)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:232)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:229)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:92)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:92)\n\tat org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:41)\n\tat org.apache.spark.sql.execution.SampleExec.doProduce(basicPhysicalOperators.scala:360)\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:97)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:232)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:229)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:92)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:92)\n\tat org.apache.spark.sql.execution.SampleExec.produce(basicPhysicalOperators.scala:324)\n\tat org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:54)\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:97)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:232)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:229)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:92)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:92)\n\tat org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:41)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen(WholeStageCodegenExec.scala:801)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:870)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:194)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:232)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:229)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:190)\n\tat org.apache.spark.sql.execution.DeserializeToObjectExec.doExecute(objects.scala:96)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:194)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:232)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:229)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:190)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:134)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:133)\n\tat org.apache.spark.sql.Dataset$RDDQueryExecution.rdd$lzycompute(Dataset.scala:3839)\n\tat org.apache.spark.sql.Dataset$RDDQueryExecution.rdd(Dataset.scala:3837)\n\tat org.apache.spark.sql.Dataset.rdd$lzycompute(Dataset.scala:3285)\n\tat org.apache.spark.sql.Dataset.rdd(Dataset.scala:3278)\n\tat org.apache.spark.ml.evaluation.BinaryClassificationEvaluator.getMetrics(BinaryClassificationEvaluator.scala:133)\n\tat org.apache.spark.ml.evaluation.BinaryClassificationEvaluator.evaluate(BinaryClassificationEvaluator.scala:100)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n\n(<class 'py4j.protocol.Py4JJavaError'>, Py4JJavaError('An error occurred while calling o5325.evaluate.\\n', JavaObject id=o5340), <traceback object at 0x7f101ff18820>)"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1639429984467_730913692",
      "id": "paragraph_1639412988109_737253415",
      "dateCreated": "2021-12-13T21:13:04+0000",
      "status": "READY",
      "$$hashKey": "object:275"
    },
    {
      "text": "%pyspark\n",
      "user": "anonymous",
      "dateUpdated": "2021-12-13T21:13:04+0000",
      "progress": 0,
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "results": {},
        "enabled": true,
        "editorSetting": {}
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1639429984467_1451644827",
      "id": "paragraph_1639420003682_1263909691",
      "dateCreated": "2021-12-13T21:13:04+0000",
      "status": "READY",
      "$$hashKey": "object:276"
    }
  ],
  "name": "Wisteria",
  "id": "2GRR7JRQT",
  "defaultInterpreterGroup": "spark",
  "version": "0.10.0",
  "noteParams": {},
  "noteForms": {},
  "angularObjects": {},
  "config": {
    "isZeppelinNotebookCronEnable": false,
    "looknfeel": "default",
    "personalizedMode": "false"
  },
  "info": {},
  "path": "/Wisteria"
}